{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19435a25-a942-434d-a535-e4abe15f4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "# Simple MLP model\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, num_classes=10):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Worker class simulating a distributed worker\n",
    "class Worker:\n",
    "    def __init__(self, worker_id, model, optimizer, loss_fn, device='cpu'):\n",
    "        self.worker_id = worker_id\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = loss_fn\n",
    "    \n",
    "    def compute_gradients(self, data, target):\n",
    "        \"\"\"Compute gradients on local data batch\"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        data, target = data.to(self.device), target.to(self.device)\n",
    "        output = self.model(data)\n",
    "        loss = self.criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Extract gradients\n",
    "        gradients = []\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                gradients.append(param.grad.clone())\n",
    "            else:\n",
    "                gradients.append(torch.zeros_like(param))\n",
    "        \n",
    "        return gradients, loss.item()\n",
    "\n",
    "# Parameter Server for synchronization\n",
    "class ParameterServer:\n",
    "    def __init__(self, model):\n",
    "        self.global_model = model\n",
    "    \n",
    "    def aggregate_gradients(self, worker_gradients: List[List[torch.Tensor]]):\n",
    "        \"\"\"Average gradients from all workers\"\"\"\n",
    "        num_workers = len(worker_gradients)\n",
    "        aggregated_grads = []\n",
    "        \n",
    "        # Average gradients across workers\n",
    "        num_params = len(worker_gradients[0])\n",
    "        for param_idx in range(num_params):\n",
    "            grad_sum = sum(worker_gradients[worker_id][param_idx] \n",
    "                          for worker_id in range(num_workers))\n",
    "            aggregated_grads.append(grad_sum / num_workers)\n",
    "        \n",
    "        return aggregated_grads\n",
    "    \n",
    "    def update_global_model(self, aggregated_grads, lr=0.01):\n",
    "        \"\"\"Update global model with aggregated gradients\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param, grad in zip(self.global_model.parameters(), aggregated_grads):\n",
    "                param -= lr * grad\n",
    "    \n",
    "    def broadcast_model(self, workers: List[Worker]):\n",
    "        \"\"\"Send updated model to all workers\"\"\"\n",
    "        for worker in workers:\n",
    "            worker.model.load_state_dict(self.global_model.state_dict())\n",
    "\n",
    "# Data-Parallel Distributed Training Simulator\n",
    "class DistributedTrainer:\n",
    "    def __init__(self, num_workers=4, device='cpu'):\n",
    "        self.num_workers = num_workers\n",
    "        self.device = device\n",
    "        \n",
    "        # Load MNIST dataset\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        \n",
    "        self.train_dataset = datasets.MNIST('./data', train=True, download=True, \n",
    "                                           transform=transform)\n",
    "        self.test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "        \n",
    "        # Create global model and parameter server\n",
    "        self.global_model = SimpleMLP().to(device)\n",
    "        self.param_server = ParameterServer(self.global_model)\n",
    "        \n",
    "        # Create workers\n",
    "        self.workers = []\n",
    "        for i in range(num_workers):\n",
    "            worker_model = SimpleMLP().to(device)\n",
    "            worker_model.load_state_dict(self.global_model.state_dict())\n",
    "            self.workers.append(Worker(i, worker_model, device))\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.test_accuracies = []\n",
    "    \n",
    "    def split_batch(self, data, target):\n",
    "        \"\"\"Split batch among workers\"\"\"\n",
    "        batch_size = data.size(0)\n",
    "        chunk_size = batch_size // self.num_workers\n",
    "        \n",
    "        data_chunks = []\n",
    "        target_chunks = []\n",
    "        \n",
    "        for i in range(self.num_workers):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = start_idx + chunk_size if i < self.num_workers - 1 else batch_size\n",
    "            data_chunks.append(data[start_idx:end_idx])\n",
    "            target_chunks.append(target[start_idx:end_idx])\n",
    "        \n",
    "        return data_chunks, target_chunks\n",
    "    \n",
    "    def train_epoch(self, dataloader):\n",
    "        \"\"\"Train for one epoch using data-parallel approach\"\"\"\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            # Step 1: Split batch among workers\n",
    "            data_chunks, target_chunks = self.split_batch(data, target)\n",
    "            \n",
    "            # Step 2: Each worker computes gradients on their chunk\n",
    "            worker_gradients = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for worker_id, worker in enumerate(self.workers):\n",
    "                grads, loss = worker.compute_gradients(\n",
    "                    data_chunks[worker_id], \n",
    "                    target_chunks[worker_id]\n",
    "                )\n",
    "                worker_gradients.append(grads)\n",
    "                batch_losses.append(loss)\n",
    "            \n",
    "            # Step 3: Parameter server aggregates gradients\n",
    "            aggregated_grads = self.param_server.aggregate_gradients(worker_gradients)\n",
    "            \n",
    "            # Step 4: Update global model\n",
    "            self.param_server.update_global_model(aggregated_grads, lr=0.01)\n",
    "            \n",
    "            # Step 5: Broadcast updated model to workers\n",
    "            self.param_server.broadcast_model(self.workers)\n",
    "            \n",
    "            epoch_loss += np.mean(batch_losses)\n",
    "            num_batches += 1\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"  Batch {batch_idx}/{len(dataloader)}, \"\n",
    "                      f\"Avg Loss: {np.mean(batch_losses):.4f}\")\n",
    "        \n",
    "        return epoch_loss / num_batches\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate global model on test set\"\"\"\n",
    "        self.global_model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=1000, shuffle=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.global_model(data)\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "                total += target.size(0)\n",
    "        \n",
    "        accuracy = 100. * correct / total\n",
    "        return accuracy\n",
    "    \n",
    "    def train(self, num_epochs=5, batch_size=256):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=batch_size, \n",
    "                                 shuffle=True)\n",
    "        \n",
    "        print(f\"Starting Data-Parallel Distributed Training\")\n",
    "        print(f\"Number of workers: {self.num_workers}\")\n",
    "        print(f\"Batch size: {batch_size} (split into {batch_size//self.num_workers} per worker)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            avg_loss = self.train_epoch(train_loader)\n",
    "            test_acc = self.evaluate()\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            self.train_losses.append(avg_loss)\n",
    "            self.test_accuracies.append(test_acc)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "            print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
    "            print(f\"  Time: {epoch_time:.2f}s\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "    def get_train_losses(self):\n",
    "        return self.train_lossess\n",
    "\n",
    "    def get_test_accuracies(self):\n",
    "        return self.test_accuracies\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Plot training curves\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a0ec7-473c-4ca6-8a59-214ea18a9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(train_lossess, test_accuracies):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Training Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(self.test_accuracies)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Test Accuracy (%)')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distributed_training_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fe895-daa7-443b-a73f-f64f054078d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create trainer with 4 workers\n",
    "trainer = DistributedTrainer(num_workers=4, device='cpu')\n",
    "\n",
    "# Train the model\n",
    "trainer.train(num_epochs=5, batch_size=256)\n",
    "\n",
    "# Plot results\n",
    "trainer.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1e0bdb-4abc-4250-bf8c-1c344dede874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
